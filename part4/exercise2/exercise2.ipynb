{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb23552-39cc-439c-ab1e-930468894129",
   "metadata": {},
   "source": [
    "# Opgave 4.2: een eenvoudig taalmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6e10-b667-4486-b952-a6dd6fde87fe",
   "metadata": {},
   "source": [
    "In deze korte opgave gaan we werken aan een eenvoudig n-gram taalmodel. Hoewel deze techniek heden ten dage grotendeels is vervangen door recurrente neurale netwerken (waar de volgende opgave over gaat), is het toch nog wel inzichtelijk om te zien hoe je met een dergelijke eenvoudige architectuur verrassende effecten kunt bereiken.\n",
    "\n",
    "Zoals tijdens het theoretisch gedeelte is toegelicht, zijn n-gram taalmodellen getraind om op basis van een input van een bepaalde hoeveelheid lettertekens (met een lengte van `n_gram`) het volgende letterteken te voorspellen. Tijdens het trainen van zo'n model wordt letter voor letter door een corpus gelopen en bijgehouden hoe vaak welke volgende letter voorkomt. Het getrainde model bestaat dat feitelijk uit een dictionary waarin de *key*s bestaan uit de mogelijke lettercombinaties uit het corpus en de *value*s uit wéér een dictionary met de daaropvolgende letters en hoe vaak die voorkomen. Het proces wordt hieronder grafisch geïllustreerdm waarbij de lengte van de `n_gram` gelijk is aan twee:\n",
    "\n",
    "![De werking van het trainen van een N-gram](./imgs/n-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b132f-6338-4c49-b0c3-8a3c0b27e0e8",
   "metadata": {},
   "source": [
    "In de cel hieronder is het staketsel van de klasse `NGramModel` gegeven. In dit initalisatie van een object van deze klasse moet meegegeven worden hoe groot de `n_gram` moet zijn, waarmee hij door een corpus moet lopen. Verder heeft deze klassen de volgende methoden:\n",
    "\n",
    "* `fit(corpus)`: hier wordt het model getraind volgens de methode die hierboven kort is beschreven.\n",
    "* `predict_proba(key)`: retourneert een dictionary de mogelijke volgende letters met hun waarschijnlijkheid, gegeven de `key`.\n",
    "* `predict(seed, length)`: retourneert een stuk tekst met lenge `length` waarvan het begin gelijk is aan `seed`.\n",
    "\n",
    "Maak de klasse `NGramModel` af. Check de tweede cel hieronder om te zien hoe hij gebruikt moet kunnen worden, inclusief een verwachte output.\n",
    "\n",
    "__Tips :__ de methode `predict` maakt gebruik van de methode `predict_proba(key)`. Je kunt hierin ook gebruik maken van [`numpy.random.choice`[(https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), die een optionele parameter `p` heeft die een waarschijnlijkheidsdistributie bevat. Let er ook op dat het mogelijk is dat `seed` niet in de getrainde data voorkomt (dus dat `predict_proba(seed)` een `None` teruggeeft."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3d68c19-7089-48ee-acd5-b3ac890b1068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T19:59:04.742599Z",
     "start_time": "2024-09-30T19:59:04.733495Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"An n-gram language model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        model (defaultdict[str, Counter]): The model that is trained on the corpus.\n",
    "        n (int): The size of the n-gram.\n",
    "\n",
    "    \"\"\"\n",
    "    model: defaultdict[str, Counter]\n",
    "    n: int\n",
    "    \n",
    "    def __init__(self, n: int = 2) -> None:\n",
    "        \"\"\"Initializes the model.\n",
    "        \n",
    "        :param n: The size of the n-gram.\n",
    "        \"\"\"\n",
    "        self.model = defaultdict(Counter)\n",
    "        self.n = n\n",
    "        \n",
    "    def fit(self, corpus: str) -> None:\n",
    "        \"\"\"Train the model on the given corpus.\n",
    "        \n",
    "        :param corpus: The text to train the model on.\n",
    "        \"\"\"\n",
    "        for i in range(len(corpus) - self.n):\n",
    "            input = corpus[i:i + self.n]\n",
    "            output = corpus[i + self.n]\n",
    "            \n",
    "            self.model[input][output] += 1\n",
    "        \n",
    "    def predic_proba(self, key: str) -> dict | None:\n",
    "        \"\"\"Predict the next letter given the key.\n",
    "        \n",
    "        :param key: The key to predict the next letter for.\n",
    "        :return: A dictionary with the possible next letters and their probabilities.\n",
    "        \"\"\"\n",
    "        if key not in self.model:\n",
    "            return None\n",
    "        \n",
    "        predictions = {}\n",
    "\n",
    "        total = sum(self.model[key].values())\n",
    "        for letter, count in self.model[key].items():\n",
    "            predictions[letter] = count / total\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    def predict(self, seed: str, length: int) -> str:\n",
    "        res = seed\n",
    "        while len(res) < length:\n",
    "            predictions = self.predic_proba(seed)\n",
    "            if predictions is None:\n",
    "                break\n",
    "\n",
    "            res += np.random.choice(list(predictions.keys()), p=list(predictions.values()))\n",
    "            seed = res[-self.n:]\n",
    "\n",
    "        return res"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "id": "21b6bc4e-ff83-42bb-bee9-76b9727125cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T19:59:42.818937Z",
     "start_time": "2024-09-30T19:59:42.760990Z"
    }
   },
   "source": [
    "with open('data/wiki.txt','r', encoding=\"utf8\") as f:\n",
    "    data = ''.join([line.strip().lower() for line in f.readlines()])\n",
    "\n",
    "model = NGramModel(4)\n",
    "model.fit(data)\n",
    "print(model.predict('afge', 300))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afgeleidt tot het gedoseerdere mogelijkheid tussen belangrijken. wanneer mogelijk, gebreideld het genmutatie van specifieke vorm van het kanker brengen;huidkankers, ook wel elektrische stralingen voorbeeldeling met namente en met zoudende chemicaliën is de organisation fouten (atoomherstoring. bij p\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8625826bf5110feb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
